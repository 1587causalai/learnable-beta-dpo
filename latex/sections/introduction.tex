% 引言章节

近年来，大型语言模型（LLMs）的发展日新月异，而如何使这些模型更好地对齐人类偏好已成为重要研究方向。Direct Preference Optimization (DPO) \cite{rafailov2023direct}作为一种直接优化语言模型的算法，因其简洁高效而广受关注。相比传统的基于人类反馈的强化学习方法（RLHF）\cite{ouyang2022training}，DPO避免了复杂的奖励模型训练和策略迭代过程，大大简化了人类偏好对齐的技术路线。

然而，标准DPO算法使用固定的β超参数来平衡参考策略和偏好学习，这导致了两个主要局限：一是上下文不敏感，无法根据不同输入内容自适应调整学习策略；二是优化效率受限，在不同难度任务上无法精细控制探索与利用的平衡。本研究旨在克服这些局限，提出一种更灵活、更有效的DPO变体算法。

本文的主要贡献包括：(1) 提出了Learnable Beta DPO算法，通过引入动态可学习的β参数实现自适应探索-利用平衡；(2) 设计了与策略模型紧密耦合的BetaHead网络，能够根据上下文动态调整β值；(3) 从信息融合的角度重新诠释了DPO算法，为理解β参数作用提供了新视角；(4) 通过大量实验验证了所提方法的有效性，展示了其在多个领域的性能优势。 