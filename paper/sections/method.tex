\subsection{BetaHead网络设计}
我们提出的Learnable Beta DPO方法核心在于设计一个与策略模型紧密耦合的BetaHead网络，用于动态计算上下文相关的$\beta$值。BetaHead网络的设计理念是：它应该能够根据模型对输入上下文的熟悉程度来调整$\beta$值，同时保持计算效率和训练稳定性。

BetaHead网络的基本结构如图\ref{fig:beta_head}所示。它接收两个关键输入：

\begin{itemize}
    \item 策略模型计算的上下文困惑度$PPL_{\pi_\theta}(x)$，反映模型对输入的确定性程度
    \item 策略模型最后一层隐状态$h_{\pi_\theta}(x)$，包含输入上下文的深层语义表征
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/beta_head_architecture.pdf}
    \caption{BetaHead网络架构。网络接收上下文困惑度和最后一层隐状态作为输入，输出动态$\beta$值。}
    \label{fig:beta_head}
\end{figure}

\subsection{动态$\beta$计算公式}
我们将动态$\beta$值设计为如下形式：

\begin{equation}
\beta(x) = w \cdot PPL_{\pi_\theta}(x) \cdot f(h_{\pi_\theta}(x))
\end{equation}

其中：
\begin{itemize}
    \item $w$是一个可学习的标量参数，控制$\beta$的整体尺度
    \item $PPL_{\pi_\theta}(x)$是上下文$x$的困惑度，定义为：
    \begin{equation}
    PPL_{\pi_\theta}(x) = \exp \left( - \frac{1}{m} \sum_{i=1}^m \log \pi_\theta(x_i | x_{<i}) \right)
    \end{equation}
    其中$m$是上下文的长度
    \item $f(h_{\pi_\theta}(x))$是一个参数化的修正因子，其取值范围限制在$[1-\epsilon, 1+\epsilon]$，$\epsilon$是一个小的常数（如0.2）。具体实现为：
    \begin{equation}
    f(h_{\pi_\theta}(x)) = 1 + \epsilon \cdot \tanh(NN(h_{\pi_\theta}(x)))
    \end{equation}
    其中$NN(\cdot)$是一个轻量级的多层感知机
\end{itemize}

这种设计有以下优势：
\begin{itemize}
    \item 困惑度$PPL_{\pi_\theta}(x)$提供了一个自动缩放机制：当模型对输入不确定（困惑度高）时，$\beta$值会增大，倾向于保守学习；当模型对输入确定（困惑度低）时，$\beta$值会减小，倾向于激进学习
    \item 修正因子$f(h_{\pi_\theta}(x))$允许模型根据上下文的语义特征微调$\beta$值，提供额外的调节灵活性
    \item 修正因子的取值范围受限，防止$\beta$值变化过大导致训练不稳定
\end{itemize}

\subsection{训练算法}
Learnable Beta DPO的训练算法如算法\ref{alg:learnable_beta_dpo}所示。与标准DPO不同，我们不仅优化策略模型的参数$\theta$，还同时优化BetaHead网络的参数$\phi$（包括标量$w$和修正因子网络$NN$的参数）。

\begin{algorithm}
\caption{Learnable Beta DPO训练算法}
\label{alg:learnable_beta_dpo}
\begin{algorithmic}[1]
\REQUIRE 参考策略 $\pi_{\text{ref}}$, 偏好数据集 $\mathcal{D}$, 初始学习率 $\eta$, BetaHead修正范围 $\epsilon$
\ENSURE 优化后的策略模型 $\pi_\theta$ 和BetaHead网络
\STATE 初始化策略模型参数 $\theta$ (从 $\pi_{\text{ref}}$ 复制)
\STATE 初始化BetaHead网络参数 $\phi$
\WHILE{未收敛}
    \STATE 从 $\mathcal{D}$ 中采样批次 $(x^{(i)}, y_w^{(i)}, y_l^{(i)})_{i=1}^B$
    \FOR{每个样本 $(x^{(i)}, y_w^{(i)}, y_l^{(i)})$}
        \STATE 计算上下文困惑度 $PPL_{\pi_\theta}(x^{(i)})$
        \STATE 提取上下文表征 $h_{\pi_\theta}(x^{(i)})$
        \STATE 通过BetaHead计算动态 $\beta(x^{(i)})$ 值
        \STATE 计算chosen样本的对数概率比 $r_w^{(i)} = \log \frac{\pi_\theta(y_w^{(i)}|x^{(i)})}{\pi_{\text{ref}}(y_w^{(i)}|x^{(i)})}$
        \STATE 计算rejected样本的对数概率比 $r_l^{(i)} = \log \frac{\pi_\theta(y_l^{(i)}|x^{(i)})}{\pi_{\text{ref}}(y_l^{(i)}|x^{(i)})}$
    \ENDFOR
    \STATE 计算批次DPO损失 $\mathcal{L} = -\frac{1}{B} \sum_{i=1}^B \log \sigma(\beta(x^{(i)}) \cdot (r_w^{(i)} - r_l^{(i)}))$
    \STATE 计算梯度 $\nabla_\theta \mathcal{L}$ 和 $\nabla_\phi \mathcal{L}$
    \STATE 更新参数 $\theta \leftarrow \theta - \eta \nabla_\theta \mathcal{L}$
    \STATE 更新参数 $\phi \leftarrow \phi - \eta \nabla_\phi \mathcal{L}$
\ENDWHILE
\end{algorithmic}
\end{algorithm}

需要注意的是，我们采用了端到端的联合优化策略，使BetaHead网络能够与策略模型协同进化。这样设计的好处是：BetaHead网络可以根据策略模型的变化动态调整$\beta$值，而策略模型也能利用更合适的$\beta$值实现更高效的学习。 