# Learnable Beta DPO 关键参考文献

## DPO及其变体

1. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. *Advances in Neural Information Processing Systems (NeurIPS)*.

2. Ethayarajh, K., Dong, H., & Ju, J. (2022). Learning to Learn from Human Feedback. *arXiv preprint arXiv:2212.04345*.

3. Casper, S., Davies, J., Shi, C., Gilbert, T. K., Lu, J., Welleck, S., Cowen-Rivers, A. I., Chan, L., & Paik, J. (2023). Open problems and fundamental limitations of reinforcement learning from human feedback. *arXiv preprint arXiv:2307.15217*.

4. Rame, A., Lu, J., Touvron, H., Dubost, M., Labeau, M., & Scialom, T. (2023). Reward rationalisation in language models. *arXiv preprint arXiv:2310.16585*.

5. Azar, M., Mnih, V., Hazan, E., Bellemare, M., & Munos, R. (2023). KL-regularized Reinforcement Learning from Human Preference. *arXiv preprint arXiv:2303.00202*.

## 信息融合与信息理论

6. Khaleghi, B., Khamis, A., Karray, F. O., & Razavi, S. N. (2013). Multisensor data fusion: A review of the state-of-the-art. *Information Fusion*, 14(1), 28-44.

7. Tishby, N., Pereira, F. C., & Bialek, W. (1999). The information bottleneck method. *arXiv preprint physics/0004057*.

8. Genewein, T., Leibfried, F., Grau-Moya, J., & Braun, D. A. (2015). Bounded rationality, abstraction, and hierarchical decision-making: An information-theoretic optimality principle. *Frontiers in Robotics and AI*, 2, 27.

## 自适应学习算法

9. Pan, Y., & Aggarwal, C. (2020). Adaptive learning of beta for recommender systems. *Proceedings of the 29th ACM International Conference on Information & Knowledge Management*, 1295-1304.

10. Zou, F., Shen, L., Jie, Z., Zhang, W., & Liu, W. (2021). A sufficient condition for convergences of adam and rmsprop. *IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*.

11. Smith, L. N. (2017). Cyclical learning rates for training neural networks. *IEEE Winter Conference on Applications of Computer Vision (WACV)*, 464-472.

## 大语言模型与人类偏好对齐

12. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems (NeurIPS)*.

13. Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., & Amodei, D. (2017). Deep reinforcement learning from human preferences. *Advances in Neural Information Processing Systems (NeurIPS)*.

14. Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., ... & Kaplan, J. (2020). Learning to summarize with human feedback. *Advances in Neural Information Processing Systems (NeurIPS)*.

15. Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. *ACM Computing Surveys*, 55(9), 1-35. 